{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a0f4e6be",
      "metadata": {
        "id": "a0f4e6be"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "from torchvision import transforms, datasets\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import platform\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For running in colab\n",
        "def in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False"
      ],
      "metadata": {
        "id": "2IL87b7aYhUK"
      },
      "id": "2IL87b7aYhUK",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d5cd6b2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5cd6b2d",
        "outputId": "ba3779ba-e072-4549-bf05-96ac45d385b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA A100-SXM4-40GB\n",
            "Using device: cuda\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"{torch.cuda.get_device_name(0)}\")\n",
        "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device.type}\\n\")\n",
        "# Print the gpu model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f00e444",
      "metadata": {
        "id": "8f00e444"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d865c28b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d865c28b",
        "outputId": "e94400d2-d873-4da7-ed7e-32e38a00e5cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Images with wrong shape: 55\n"
          ]
        }
      ],
      "source": [
        "# Find images that are the wrong shape\n",
        "categories = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
        "dirs = []\n",
        "\n",
        "if in_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    dirs = ['/content/drive/MyDrive/2025-2026/Semester 1/MTH 4320/data/seg_test/seg_test/' + category for category in categories]\n",
        "    dirs.extend(['/content/drive/MyDrive/2025-2026/Semester 1/MTH 4320/data/seg_train/seg_train/' + category for category in categories])\n",
        "else:\n",
        "    dirs = ['data/seg_test/seg_test/' + category for category in categories]\n",
        "    dirs.extend(['data/seg_train/seg_train/' + category for category in categories])\n",
        "\n",
        "counter = 0\n",
        "for dir in dirs:\n",
        "    for image in os.listdir(dir):\n",
        "        image_path = os.path.join(dir, image)\n",
        "        if image_path.endswith(('.jpg', 'jpeg', '.png')):\n",
        "            img = Image.open(image_path)\n",
        "            if img.size != (150, 150):\n",
        "                os.remove(image_path)\n",
        "                counter += 1\n",
        "print(f\"Images with wrong shape: {counter}\") # originally 55\n",
        "# 4 minutes on google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6f282289",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f282289",
        "outputId": "54ec7c3d-b62d-4527-9330-494c83f54f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
            "Train size: 11188\n",
            "Val size: 2798\n",
            "Test size: 2993\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "if in_colab():\n",
        "    dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/2025-2026/Semester 1/MTH 4320/data/seg_train/seg_train/\", transform=transforms.ToTensor())\n",
        "    test_dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/2025-2026/Semester 1/MTH 4320/data/seg_test/seg_test/\", transform=transforms.ToTensor())\n",
        "else:\n",
        "    dataset = datasets.ImageFolder(root=\"data/seg_train/seg_train\", transform=transforms.ToTensor())\n",
        "    test_dataset = datasets.ImageFolder(root=\"data/seg_test/seg_test\", transform=transforms.ToTensor())\n",
        "\n",
        "print(f\"Classes: {dataset.classes}\")\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Val size: {len(val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "# Data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "712aa5ca",
      "metadata": {
        "id": "712aa5ca"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dc8947bf",
      "metadata": {
        "id": "dc8947bf"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy from logits\n",
        "def accuracy_from_logits(logits, y):\n",
        "    preds = logits.argmax(1)  # choose class with highest predicted score\n",
        "    return (preds == y).float().mean().item()  # fraction of correct predictions\n",
        "\n",
        "def train(name, model, optimizer, epochs, criterion, history = {\n",
        "    \"train_loss\": [], \"train_acc\": [],\n",
        "    \"val_loss\":   [], \"val_acc\":   []\n",
        "}):\n",
        "    # Early stopping settings\n",
        "    patience = 10          # epochs to wait after last improvement\n",
        "    min_delta = 0.0        # minimum change in val_loss to qualify as improvement\n",
        "    best_val = 10e20       # track best validation loss\n",
        "    best_epoch = -1\n",
        "    patience_ctr = 0\n",
        "    best_ckpt_path = \"best.pt\"\n",
        "\n",
        "    train_start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss, running_correct, total = 0.0, 0, 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * x.size(0)\n",
        "            running_correct += (logits.argmax(1) == y).sum().item()\n",
        "            total += x.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc  = running_correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_running_loss, val_running_correct, val_total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "                val_running_loss += loss.item() * x.size(0)\n",
        "                val_running_correct += (logits.argmax(1) == y).sum().item()\n",
        "                val_total += x.size(0)\n",
        "\n",
        "        val_loss = val_running_loss / val_total\n",
        "        val_acc  = val_running_correct / val_total\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        print(f\"Epoch {epoch:02d} | \"\n",
        "            f\"train: loss={train_loss:.4f}, acc={train_acc:.4f} | \"\n",
        "            f\"val: loss={val_loss:.4f}, acc={val_acc:.4f} | \"\n",
        "            f\"time: {epoch_time:.2f}s\")\n",
        "\n",
        "        # Early stopping check (monitor val_loss)\n",
        "        if val_loss < best_val - min_delta:\n",
        "            best_val = val_loss\n",
        "            best_epoch = epoch\n",
        "            patience_ctr = 0\n",
        "            # Save best checkpoint so far\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"history\": history,\n",
        "                \"best_val_loss\": best_val\n",
        "            }, best_ckpt_path)\n",
        "            print(f\"  -> New best val_loss {best_val:.4f} at epoch {epoch}. Saved to {best_ckpt_path}.\")\n",
        "        else:\n",
        "            patience_ctr += 1\n",
        "            if patience_ctr >= patience:\n",
        "                print(f\"\\nEarly stopping triggered at epoch {epoch} \"\n",
        "                    f\"(no improvement for {patience} epochs). Best epoch: {best_epoch}.\")\n",
        "                break\n",
        "\n",
        "    # Total training time\n",
        "    total_time = time.time() - train_start_time\n",
        "    print(f\"\\nTotal training time: {total_time:.2f}s\")\n",
        "    print(f\"Best epoch: {best_epoch} | Best val_loss: {best_val:.4f}\")\n",
        "\n",
        "    # Restore best model before final save (in case we stopped after it)\n",
        "    ckpt = torch.load(best_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "    # Save final artifact (model + history)\n",
        "    final_path = f\"models/{name}.pth\"\n",
        "    torch.save({\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"history\": history,\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"best_val_loss\": best_val\n",
        "    }, final_path)\n",
        "    print(f\"Final model saved to '{final_path}'\")\n",
        "\n",
        "def display_results(path, model):\n",
        "    # Loss and accuracy graph\n",
        "    # Load the trained model from file\n",
        "    #model = MLP(128*128).to(device)  # initialize model structure\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    history = checkpoint[\"history\"]\n",
        "\n",
        "    model.eval()  # set to evaluation mode\n",
        "\n",
        "    # Prepare x-axis for plots\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    # Create figure and first axis (loss)\n",
        "    fig, ax1 = plt.subplots()\n",
        "\n",
        "    color_loss = \"tab:blue\"\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\", color=color_loss)\n",
        "    ax1.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", color=color_loss, linestyle=\"-\")\n",
        "    ax1.plot(epochs, history[\"val_loss\"], label=\"Val Loss\", color=color_loss, linestyle=\"--\")\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=color_loss)\n",
        "\n",
        "    # Second y-axis for accuracy\n",
        "    ax2 = ax1.twinx()\n",
        "    color_acc = \"tab:orange\"\n",
        "    ax2.set_ylabel(\"Accuracy\", color=color_acc)\n",
        "    ax2.plot(epochs, history[\"train_acc\"], label=\"Train Acc\", color=color_acc, linestyle=\"-\")\n",
        "    ax2.plot(epochs, history[\"val_acc\"], label=\"Val Acc\", color=color_acc, linestyle=\"--\")\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=color_acc)\n",
        "\n",
        "    # Combine legends from both axes\n",
        "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"center right\")\n",
        "\n",
        "    plt.title(\"Loss and Accuracy over Epochs\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Confusion matrix\n",
        "    # Variables to track accuracy\n",
        "    test_correct, test_total = 0, 0\n",
        "\n",
        "    # Lists to store predictions and true labels for the confusion matrix\n",
        "    all_preds, all_trues = [], []\n",
        "\n",
        "    # Disable gradient computation for faster evaluation\n",
        "    with torch.no_grad():\n",
        "        # Loop through the test set in batches\n",
        "        for x, y in test_loader:\n",
        "            # Move inputs to the computation device (CPU/GPU/MPS)\n",
        "            x = x.to(device)\n",
        "\n",
        "            # Forward pass to get raw model outputs (logits)\n",
        "            logits = model(x)\n",
        "\n",
        "            # Get predicted class indices (highest logit per sample)\n",
        "            preds = logits.argmax(1).cpu().numpy()\n",
        "\n",
        "            # Store predictions and ground truth labels for later\n",
        "            all_preds.append(preds)\n",
        "            all_trues.append(y.numpy())\n",
        "\n",
        "            # Update accuracy counters\n",
        "            test_correct += (preds == y.numpy()).sum()\n",
        "            test_total   += y.size(0)\n",
        "\n",
        "    # Compute overall test accuracy\n",
        "    test_acc = test_correct / test_total\n",
        "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Flatten predictions and true labels into 1D arrays\n",
        "    y_true = np.concatenate(all_trues).ravel()\n",
        "    y_pred = np.concatenate(all_preds).ravel()\n",
        "\n",
        "    # Dynamically detect classes from both true and predicted\n",
        "    class_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
        "    print(f\"Detected {len(class_labels)} classes: {class_labels}\")\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n",
        "\n",
        "    # Plot annotated confusion matrix\n",
        "    plt.figure(figsize=(6,6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,              # annotate all cells\n",
        "        fmt=\"d\",                 # integer format\n",
        "        cbar=True,\n",
        "        xticklabels=class_labels,\n",
        "        yticklabels=class_labels\n",
        "    )\n",
        "    plt.title(\"Confusion Matrix (Test)\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b31a8d0",
      "metadata": {
        "id": "7b31a8d0"
      },
      "source": [
        "## 1st Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "995ec68e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "995ec68e",
        "outputId": "ef659249-313a-49a8-d563-8fdfb99a36e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train: loss=144.7528, acc=0.2332 | val: loss=1.7779, acc=0.2009 | time: 54.52s\n",
            "  -> New best val_loss 1.7779 at epoch 1. Saved to best.pt.\n",
            "Epoch 02 | train: loss=1.5670, acc=0.3292 | val: loss=1.3272, acc=0.4464 | time: 42.76s\n",
            "  -> New best val_loss 1.3272 at epoch 2. Saved to best.pt.\n",
            "Epoch 03 | train: loss=1.2755, acc=0.4806 | val: loss=1.1603, acc=0.5329 | time: 42.77s\n",
            "  -> New best val_loss 1.1603 at epoch 3. Saved to best.pt.\n",
            "Epoch 04 | train: loss=1.0748, acc=0.5745 | val: loss=1.0383, acc=0.5904 | time: 42.61s\n",
            "  -> New best val_loss 1.0383 at epoch 4. Saved to best.pt.\n",
            "Epoch 05 | train: loss=0.9685, acc=0.6194 | val: loss=1.0292, acc=0.5947 | time: 42.26s\n",
            "  -> New best val_loss 1.0292 at epoch 5. Saved to best.pt.\n",
            "Epoch 06 | train: loss=0.8745, acc=0.6566 | val: loss=1.0261, acc=0.5994 | time: 42.30s\n",
            "  -> New best val_loss 1.0261 at epoch 6. Saved to best.pt.\n",
            "Epoch 07 | train: loss=0.7984, acc=0.6894 | val: loss=1.0678, acc=0.5961 | time: 41.77s\n",
            "Epoch 08 | train: loss=0.7175, acc=0.7235 | val: loss=1.1385, acc=0.5818 | time: 41.36s\n",
            "Epoch 09 | train: loss=0.6272, acc=0.7587 | val: loss=1.2592, acc=0.5718 | time: 41.01s\n",
            "Epoch 10 | train: loss=0.5273, acc=0.8019 | val: loss=1.3233, acc=0.5815 | time: 41.16s\n",
            "Epoch 11 | train: loss=0.4327, acc=0.8388 | val: loss=1.5724, acc=0.5540 | time: 41.13s\n",
            "Epoch 12 | train: loss=0.3515, acc=0.8699 | val: loss=1.8357, acc=0.5658 | time: 40.35s\n",
            "Epoch 13 | train: loss=0.3095, acc=0.8920 | val: loss=1.8940, acc=0.5479 | time: 40.36s\n",
            "Epoch 14 | train: loss=0.2901, acc=0.8973 | val: loss=2.0898, acc=0.5411 | time: 40.99s\n",
            "Epoch 15 | train: loss=0.2213, acc=0.9228 | val: loss=2.3018, acc=0.5547 | time: 40.73s\n",
            "Epoch 16 | train: loss=0.1964, acc=0.9336 | val: loss=2.7045, acc=0.5293 | time: 40.59s\n",
            "\n",
            "Early stopping triggered at epoch 16 (no improvement for 10 epochs). Best epoch: 6.\n",
            "\n",
            "Total training time: 682.92s\n",
            "Best epoch: 6 | Best val_loss: 1.0261\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory models does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4063672624.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2900613767.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(name, model, optimizer, epochs, criterion, history)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Save final artifact (model + history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mfinal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"models/{name}.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     torch.save({\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"model_state\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m\"history\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             _save(\n\u001b[1;32m    968\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             super().__init__(\n\u001b[0;32m--> 792\u001b[0;31m                 torch._C.PyTorchFileWriter(\n\u001b[0m\u001b[1;32m    793\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_crc32_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_storage_alignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory models does not exist."
          ]
        }
      ],
      "source": [
        "class CNN1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 18 * 18, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 6)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "name = \"CNN1\"\n",
        "model = CNN1().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train(name, model, optimizer, epochs, criterion)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}