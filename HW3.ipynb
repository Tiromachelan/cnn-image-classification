{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a0f4e6be",
      "metadata": {
        "id": "a0f4e6be"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "from torchvision import transforms, datasets\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import platform\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For running in colab\n",
        "def in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False"
      ],
      "metadata": {
        "id": "2IL87b7aYhUK"
      },
      "id": "2IL87b7aYhUK",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d5cd6b2d",
      "metadata": {
        "id": "d5cd6b2d",
        "outputId": "ba3779ba-e072-4549-bf05-96ac45d385b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA A100-SXM4-40GB\n",
            "Using device: cuda\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"{torch.cuda.get_device_name(0)}\")\n",
        "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device.type}\\n\")\n",
        "# Print the gpu model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f00e444",
      "metadata": {
        "id": "8f00e444"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d865c28b",
      "metadata": {
        "id": "d865c28b",
        "outputId": "e94400d2-d873-4da7-ed7e-32e38a00e5cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Images with wrong shape: 55\n"
          ]
        }
      ],
      "source": [
        "# Find images that are the wrong shape\n",
        "categories = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
        "dirs = []\n",
        "\n",
        "if in_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    dirs = ['/content/drive/MyDrive/2025-2026/Semester 1/MTH 4320/data/seg_test/seg_test/' + category for category in categories]\n",
        "    dirs.extend(['/content/drive/MyDrive/2025-2026/Semester 1/MTH 4320/data/seg_train/seg_train/' + category for category in categories])\n",
        "else:\n",
        "    dirs = ['data/seg_test/seg_test/' + category for category in categories]\n",
        "    dirs.extend(['data/seg_train/seg_train/' + category for category in categories])\n",
        "\n",
        "counter = 0\n",
        "for dir in dirs:\n",
        "    for image in os.listdir(dir):\n",
        "        image_path = os.path.join(dir, image)\n",
        "        if image_path.endswith(('.jpg', 'jpeg', '.png')):\n",
        "            img = Image.open(image_path)\n",
        "            if img.size != (150, 150):\n",
        "                os.remove(image_path)\n",
        "                counter += 1\n",
        "print(f\"Images with wrong shape: {counter}\") # originally 55\n",
        "# 4 minutes on google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6f282289",
      "metadata": {
        "id": "6f282289",
        "outputId": "54ec7c3d-b62d-4527-9330-494c83f54f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
            "Train size: 11188\n",
            "Val size: 2798\n",
            "Test size: 2993\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "if in_colab():\n",
        "    dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/2025-2026/Semester 1/MTH 4320/data/seg_train/seg_train/\", transform=transforms.ToTensor())\n",
        "    test_dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/2025-2026/Semester 1/MTH 4320/data/seg_test/seg_test/\", transform=transforms.ToTensor())\n",
        "else:\n",
        "    dataset = datasets.ImageFolder(root=\"data/seg_train/seg_train\", transform=transforms.ToTensor())\n",
        "    test_dataset = datasets.ImageFolder(root=\"data/seg_test/seg_test\", transform=transforms.ToTensor())\n",
        "\n",
        "print(f\"Classes: {dataset.classes}\")\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Val size: {len(val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "# Data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "712aa5ca",
      "metadata": {
        "id": "712aa5ca"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8947bf",
      "metadata": {
        "id": "dc8947bf"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy from logits\n",
        "def accuracy_from_logits(logits, y):\n",
        "    preds = logits.argmax(1)  # choose class with highest predicted score\n",
        "    return (preds == y).float().mean().item()  # fraction of correct predictions\n",
        "\n",
        "def train(name, model, optimizer, epochs, criterion, history = {\n",
        "    \"train_loss\": [], \"train_acc\": [],\n",
        "    \"val_loss\":   [], \"val_acc\":   []\n",
        "}):\n",
        "    # Early stopping settings\n",
        "    patience = 10          # epochs to wait after last improvement\n",
        "    min_delta = 0.0        # minimum change in val_loss to qualify as improvement\n",
        "    best_val = 10e20       # track best validation loss\n",
        "    best_epoch = -1\n",
        "    patience_ctr = 0\n",
        "    best_ckpt_path = \"best.pt\"\n",
        "\n",
        "    train_start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss, running_correct, total = 0.0, 0, 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * x.size(0)\n",
        "            running_correct += (logits.argmax(1) == y).sum().item()\n",
        "            total += x.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc  = running_correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_running_loss, val_running_correct, val_total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "                val_running_loss += loss.item() * x.size(0)\n",
        "                val_running_correct += (logits.argmax(1) == y).sum().item()\n",
        "                val_total += x.size(0)\n",
        "\n",
        "        val_loss = val_running_loss / val_total\n",
        "        val_acc  = val_running_correct / val_total\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        print(f\"Epoch {epoch:02d} | \"\n",
        "            f\"train: loss={train_loss:.4f}, acc={train_acc:.4f} | \"\n",
        "            f\"val: loss={val_loss:.4f}, acc={val_acc:.4f} | \"\n",
        "            f\"time: {epoch_time:.2f}s\")\n",
        "\n",
        "        # Early stopping check (monitor val_loss)\n",
        "        if val_loss < best_val - min_delta:\n",
        "            best_val = val_loss\n",
        "            best_epoch = epoch\n",
        "            patience_ctr = 0\n",
        "            # Save best checkpoint so far\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"history\": history,\n",
        "                \"best_val_loss\": best_val\n",
        "            }, best_ckpt_path)\n",
        "            print(f\"  -> New best val_loss {best_val:.4f} at epoch {epoch}. Saved to {best_ckpt_path}.\")\n",
        "        else:\n",
        "            patience_ctr += 1\n",
        "            if patience_ctr >= patience:\n",
        "                print(f\"\\nEarly stopping triggered at epoch {epoch} \"\n",
        "                    f\"(no improvement for {patience} epochs). Best epoch: {best_epoch}.\")\n",
        "                break\n",
        "\n",
        "    # Total training time\n",
        "    total_time = time.time() - train_start_time\n",
        "    print(f\"\\nTotal training time: {total_time:.2f}s\")\n",
        "    print(f\"Best epoch: {best_epoch} | Best val_loss: {best_val:.4f}\")\n",
        "\n",
        "    # Restore best model before final save (in case we stopped after it)\n",
        "    ckpt = torch.load(best_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "    # Save final artifact (model + history)\n",
        "    final_path = f\"models/{name}.pth\"\n",
        "    torch.save({\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"history\": history,\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"best_val_loss\": best_val\n",
        "    }, final_path)\n",
        "    print(f\"Final model saved to '{final_path}'\")\n",
        "\n",
        "def display_results(path, model):\n",
        "    # Loss and accuracy graph\n",
        "    # Load the trained model from file\n",
        "    #model = MLP(128*128).to(device)  # initialize model structure\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    history = checkpoint[\"history\"]\n",
        "\n",
        "    model.eval()  # set to evaluation mode\n",
        "\n",
        "    # Prepare x-axis for plots\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    # Create figure and first axis (loss)\n",
        "    fig, ax1 = plt.subplots()\n",
        "\n",
        "    color_loss = \"tab:blue\"\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\", color=color_loss)\n",
        "    ax1.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", color=color_loss, linestyle=\"-\")\n",
        "    ax1.plot(epochs, history[\"val_loss\"], label=\"Val Loss\", color=color_loss, linestyle=\"--\")\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=color_loss)\n",
        "\n",
        "    # Second y-axis for accuracy\n",
        "    ax2 = ax1.twinx()\n",
        "    color_acc = \"tab:orange\"\n",
        "    ax2.set_ylabel(\"Accuracy\", color=color_acc)\n",
        "    ax2.plot(epochs, history[\"train_acc\"], label=\"Train Acc\", color=color_acc, linestyle=\"-\")\n",
        "    ax2.plot(epochs, history[\"val_acc\"], label=\"Val Acc\", color=color_acc, linestyle=\"--\")\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=color_acc)\n",
        "\n",
        "    # Combine legends from both axes\n",
        "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"center right\")\n",
        "\n",
        "    plt.title(\"Loss and Accuracy over Epochs\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Confusion matrix\n",
        "    # Variables to track accuracy\n",
        "    test_correct, test_total = 0, 0\n",
        "\n",
        "    # Lists to store predictions and true labels for the confusion matrix\n",
        "    all_preds, all_trues = [], []\n",
        "\n",
        "    # Disable gradient computation for faster evaluation\n",
        "    with torch.no_grad():\n",
        "        # Loop through the test set in batches\n",
        "        for x, y in test_loader:\n",
        "            # Move inputs to the computation device (CPU/GPU/MPS)\n",
        "            x = x.to(device)\n",
        "\n",
        "            # Forward pass to get raw model outputs (logits)\n",
        "            logits = model(x)\n",
        "\n",
        "            # Get predicted class indices (highest logit per sample)\n",
        "            preds = logits.argmax(1).cpu().numpy()\n",
        "\n",
        "            # Store predictions and ground truth labels for later\n",
        "            all_preds.append(preds)\n",
        "            all_trues.append(y.numpy())\n",
        "\n",
        "            # Update accuracy counters\n",
        "            test_correct += (preds == y.numpy()).sum()\n",
        "            test_total   += y.size(0)\n",
        "\n",
        "    # Compute overall test accuracy\n",
        "    test_acc = test_correct / test_total\n",
        "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Flatten predictions and true labels into 1D arrays\n",
        "    y_true = np.concatenate(all_trues).ravel()\n",
        "    y_pred = np.concatenate(all_preds).ravel()\n",
        "\n",
        "    # Dynamically detect classes from both true and predicted\n",
        "    class_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
        "    print(f\"Detected {len(class_labels)} classes: {class_labels}\")\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n",
        "\n",
        "    # Plot annotated confusion matrix\n",
        "    plt.figure(figsize=(6,6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,              # annotate all cells\n",
        "        fmt=\"d\",                 # integer format\n",
        "        cbar=True,\n",
        "        xticklabels=class_labels,\n",
        "        yticklabels=class_labels\n",
        "    )\n",
        "    plt.title(\"Confusion Matrix (Test)\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b31a8d0",
      "metadata": {
        "id": "7b31a8d0"
      },
      "source": [
        "## 1st Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "995ec68e",
      "metadata": {
        "id": "995ec68e"
      },
      "outputs": [],
      "source": [
        "class CNN1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 18 * 18, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 6)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "name = \"CNN1\"\n",
        "model = CNN1().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train(name, model, optimizer, epochs, criterion)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}