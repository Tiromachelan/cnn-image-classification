{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0f4e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d5cd6b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device.type}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00e444",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images with wrong shape: 55\n"
     ]
    }
   ],
   "source": [
    "# Find images that are the wrong shape\n",
    "categories = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "dirs = ['data/seg_test/seg_test/' + category for category in categories]\n",
    "dirs.extend(['data/seg_train/seg_train/' + category for category in categories])\n",
    "\n",
    "counter = 0\n",
    "for dir in dirs:\n",
    "    for image in os.listdir(dir):\n",
    "        image_path = os.path.join(dir, image)\n",
    "        if image_path.endswith(('.jpg', 'jpeg', '.png')):\n",
    "            img = Image.open(image_path)\n",
    "            if img.size != (150, 150):\n",
    "\n",
    "                counter += 1\n",
    "print(f\"Images with wrong shape: {counter}\") # 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f282289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
      "Train size: 11227\n",
      "Val size: 2807\n",
      "Test size: 3000\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset = datasets.ImageFolder(root=\"data/seg_train/seg_train\", transform=transforms.ToTensor())\n",
    "test_dataset = datasets.ImageFolder(root=\"data/seg_test/seg_test\", transform=transforms.ToTensor())\n",
    "\n",
    "print(f\"Classes: {dataset.classes}\")\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Val size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712aa5ca",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc8947bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy from logits\n",
    "def accuracy_from_logits(logits, y):\n",
    "    preds = logits.argmax(1)  # choose class with highest predicted score\n",
    "    return (preds == y).float().mean().item()  # fraction of correct predictions\n",
    "\n",
    "def train(name, model, optimizer, epochs, criterion, history = {\n",
    "    \"train_loss\": [], \"train_acc\": [],\n",
    "    \"val_loss\":   [], \"val_acc\":   []\n",
    "}):\n",
    "    # Early stopping settings\n",
    "    patience = 10          # epochs to wait after last improvement\n",
    "    min_delta = 0.0        # minimum change in val_loss to qualify as improvement\n",
    "    best_val = 10e20       # track best validation loss\n",
    "    best_epoch = -1\n",
    "    patience_ctr = 0\n",
    "    best_ckpt_path = \"best.pt\"\n",
    "\n",
    "    train_start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss, running_correct, total = 0.0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            running_correct += (logits.argmax(1) == y).sum().item()\n",
    "            total += x.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc  = running_correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss, val_running_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                val_running_loss += loss.item() * x.size(0)\n",
    "                val_running_correct += (logits.argmax(1) == y).sum().item()\n",
    "                val_total += x.size(0)\n",
    "\n",
    "        val_loss = val_running_loss / val_total\n",
    "        val_acc  = val_running_correct / val_total\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch:02d} | \"\n",
    "            f\"train: loss={train_loss:.4f}, acc={train_acc:.4f} | \"\n",
    "            f\"val: loss={val_loss:.4f}, acc={val_acc:.4f} | \"\n",
    "            f\"time: {epoch_time:.2f}s\")\n",
    "\n",
    "        # Early stopping check (monitor val_loss)\n",
    "        if val_loss < best_val - min_delta:\n",
    "            best_val = val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_ctr = 0\n",
    "            # Save best checkpoint so far\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"history\": history,\n",
    "                \"best_val_loss\": best_val\n",
    "            }, best_ckpt_path)\n",
    "            print(f\"  -> New best val_loss {best_val:.4f} at epoch {epoch}. Saved to {best_ckpt_path}.\")\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch} \"\n",
    "                    f\"(no improvement for {patience} epochs). Best epoch: {best_epoch}.\")\n",
    "                break\n",
    "\n",
    "    # Total training time\n",
    "    total_time = time.time() - train_start_time\n",
    "    print(f\"\\nTotal training time: {total_time:.2f}s\")\n",
    "    print(f\"Best epoch: {best_epoch} | Best val_loss: {best_val:.4f}\")\n",
    "\n",
    "    # Restore best model before final save (in case we stopped after it)\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "\n",
    "    # Save final artifact (model + history)\n",
    "    final_path = f\"models/{name}.pth\"\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"history\": history,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_val_loss\": best_val\n",
    "    }, final_path)\n",
    "    print(f\"Final model saved to '{final_path}'\")\n",
    "\n",
    "def display_results(path, model):\n",
    "    # Loss and accuracy graph\n",
    "    # Load the trained model from file\n",
    "    #model = MLP(128*128).to(device)  # initialize model structure\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    history = checkpoint[\"history\"]\n",
    "\n",
    "    model.eval()  # set to evaluation mode\n",
    "\n",
    "    # Prepare x-axis for plots\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Create figure and first axis (loss)\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color_loss = \"tab:blue\"\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\", color=color_loss)\n",
    "    ax1.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", color=color_loss, linestyle=\"-\")\n",
    "    ax1.plot(epochs, history[\"val_loss\"], label=\"Val Loss\", color=color_loss, linestyle=\"--\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color_loss)\n",
    "\n",
    "    # Second y-axis for accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    color_acc = \"tab:orange\"\n",
    "    ax2.set_ylabel(\"Accuracy\", color=color_acc)\n",
    "    ax2.plot(epochs, history[\"train_acc\"], label=\"Train Acc\", color=color_acc, linestyle=\"-\")\n",
    "    ax2.plot(epochs, history[\"val_acc\"], label=\"Val Acc\", color=color_acc, linestyle=\"--\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=color_acc)\n",
    "\n",
    "    # Combine legends from both axes\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"center right\")\n",
    "\n",
    "    plt.title(\"Loss and Accuracy over Epochs\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Confusion matrix\n",
    "    # Variables to track accuracy\n",
    "    test_correct, test_total = 0, 0\n",
    "\n",
    "    # Lists to store predictions and true labels for the confusion matrix\n",
    "    all_preds, all_trues = [], []\n",
    "\n",
    "    # Disable gradient computation for faster evaluation\n",
    "    with torch.no_grad():\n",
    "        # Loop through the test set in batches\n",
    "        for x, y in test_loader:\n",
    "            # Move inputs to the computation device (CPU/GPU/MPS)\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # Forward pass to get raw model outputs (logits)\n",
    "            logits = model(x)\n",
    "            \n",
    "            # Get predicted class indices (highest logit per sample)\n",
    "            preds = logits.argmax(1).cpu().numpy()\n",
    "            \n",
    "            # Store predictions and ground truth labels for later\n",
    "            all_preds.append(preds)\n",
    "            all_trues.append(y.numpy())\n",
    "            \n",
    "            # Update accuracy counters\n",
    "            test_correct += (preds == y.numpy()).sum()\n",
    "            test_total   += y.size(0)\n",
    "\n",
    "    # Compute overall test accuracy\n",
    "    test_acc = test_correct / test_total\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Flatten predictions and true labels into 1D arrays\n",
    "    y_true = np.concatenate(all_trues).ravel()\n",
    "    y_pred = np.concatenate(all_preds).ravel()\n",
    "\n",
    "    # Dynamically detect classes from both true and predicted\n",
    "    class_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    print(f\"Detected {len(class_labels)} classes: {class_labels}\")\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n",
    "\n",
    "    # Plot annotated confusion matrix\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,              # annotate all cells\n",
    "        fmt=\"d\",                 # integer format\n",
    "        cbar=True,\n",
    "        xticklabels=class_labels,\n",
    "        yticklabels=class_labels\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix (Test)\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b31a8d0",
   "metadata": {},
   "source": [
    "## 1st Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "995ec68e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 150, 150] at entry 0 and [3, 76, 150] at entry 98",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m epochs = \u001b[32m100\u001b[39m\n\u001b[32m     26\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(name, model, optimizer, epochs, criterion, history)\u001b[39m\n\u001b[32m     24\u001b[39m model.train()\n\u001b[32m     25\u001b[39m running_loss, running_correct, total = \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_to_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects each tensor to be equal size, but got [3, 150, 150] at entry 0 and [3, 76, 150] at entry 98"
     ]
    }
   ],
   "source": [
    "class CNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 16 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 6)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "name = \"CNN1\"\n",
    "model = CNN1().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(name, model, optimizer, epochs, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
